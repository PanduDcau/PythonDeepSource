{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#5D73F2; color:#19180F; font-size:40px; font-family:Arial; padding:10px; border: 5px solid #19180F; border-radius:10px\"> Emotion Recognition using Deep Emotion </div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nImporting modules  </div>","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nimport glob\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:05:10.909683Z","iopub.execute_input":"2023-07-15T20:05:10.910448Z","iopub.status.idle":"2023-07-15T20:05:14.727278Z","shell.execute_reply.started":"2023-07-15T20:05:10.910404Z","shell.execute_reply":"2023-07-15T20:05:14.725909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nSelecting device    </div>","metadata":{}},{"cell_type":"code","source":"print(torch.cuda.is_available())\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:05:14.734027Z","iopub.execute_input":"2023-07-15T20:05:14.734833Z","iopub.status.idle":"2023-07-15T20:05:14.812770Z","shell.execute_reply.started":"2023-07-15T20:05:14.734767Z","shell.execute_reply":"2023-07-15T20:05:14.810380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nInspecting class names    </div>","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/fer2013/\"\nclass_names = os.listdir(dataset_path+\"/train\")\nprint(class_names)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:05:14.814083Z","iopub.execute_input":"2023-07-15T20:05:14.814752Z","iopub.status.idle":"2023-07-15T20:05:14.840565Z","shell.execute_reply.started":"2023-07-15T20:05:14.814694Z","shell.execute_reply":"2023-07-15T20:05:14.839059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nDisplaying one image for each emotion in train set    </div>","metadata":{}},{"cell_type":"code","source":"emotions = []\nfor file_name in glob.glob(dataset_path+'/train/*/*'):\n    emotion = file_name.split('/')[-2]\n    if emotion not in emotions:\n        img = cv2.imread(file_name)\n        plt.imshow(img)\n        plt.title(\"Displaying {} emotion\".format(emotion))\n        plt.show()\n    emotions.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:05:14.843884Z","iopub.execute_input":"2023-07-15T20:05:14.844382Z","iopub.status.idle":"2023-07-15T20:05:24.061836Z","shell.execute_reply.started":"2023-07-15T20:05:14.844339Z","shell.execute_reply":"2023-07-15T20:05:24.060633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nVisualizing one image for each emotion in val set    </div>","metadata":{}},{"cell_type":"code","source":"emotions = []\nfor file_name in glob.glob(dataset_path+'/test/*/*'):\n    emotion = file_name.split('/')[-2]\n    if emotion not in emotions:\n        img = cv2.imread(file_name)\n        plt.imshow(img)\n        plt.title(\"Displaying {} emotion\".format(emotion))\n        plt.show()\n    emotions.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:05:24.063756Z","iopub.execute_input":"2023-07-15T20:05:24.064533Z","iopub.status.idle":"2023-07-15T20:05:26.405309Z","shell.execute_reply.started":"2023-07-15T20:05:24.064491Z","shell.execute_reply":"2023-07-15T20:05:26.404193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nDefining transforms    </div>","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((48, 48))])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:05:26.406978Z","iopub.execute_input":"2023-07-15T20:05:26.407358Z","iopub.status.idle":"2023-07-15T20:05:26.414485Z","shell.execute_reply.started":"2023-07-15T20:05:26.407318Z","shell.execute_reply":"2023-07-15T20:05:26.413461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nCreating dataset and dataloader instance    </div>","metadata":{}},{"cell_type":"code","source":"train_dataset = ImageFolder(dataset_path+'/train',transform)\ntrain_loader = DataLoader(dataset=train_dataset,batch_size=2048*6)\n#creating val data loaders\nval_dataset = ImageFolder(dataset_path+'/test',transform)\nval_loader = DataLoader(dataset=val_dataset,batch_size=2048)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:05:26.416112Z","iopub.execute_input":"2023-07-15T20:05:26.416722Z","iopub.status.idle":"2023-07-15T20:06:05.404640Z","shell.execute_reply.started":"2023-07-15T20:05:26.416641Z","shell.execute_reply":"2023-07-15T20:06:05.403431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nPerforming sanity check of train loader</div>","metadata":{}},{"cell_type":"code","source":"for batch in train_loader:\n    print(batch[0].shape,batch[1].shape)\n    break\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:06:05.406055Z","iopub.execute_input":"2023-07-15T20:06:05.406442Z","iopub.status.idle":"2023-07-15T20:07:35.500039Z","shell.execute_reply.started":"2023-07-15T20:06:05.406400Z","shell.execute_reply":"2023-07-15T20:07:35.498870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nEnsuring reversed class mappings    </div>","metadata":{}},{"cell_type":"code","source":"classes_mappings = train_dataset.class_to_idx\n#reversing\nreversed_mappings = {v:k for k,v in classes_mappings.items()}\nprint(reversed_mappings)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:07:35.501920Z","iopub.execute_input":"2023-07-15T20:07:35.502330Z","iopub.status.idle":"2023-07-15T20:07:35.508715Z","shell.execute_reply.started":"2023-07-15T20:07:35.502287Z","shell.execute_reply":"2023-07-15T20:07:35.507611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nDefining classification base class    </div>","metadata":{}},{"cell_type":"code","source":"class FaceEmotionClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch\n        images= images.to(device)\n        labels= labels.to(device) \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch\n        images= images.to(device)\n        labels= labels.to(device) \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}],  train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:07:35.512915Z","iopub.execute_input":"2023-07-15T20:07:35.513581Z","iopub.status.idle":"2023-07-15T20:07:35.526574Z","shell.execute_reply.started":"2023-07-15T20:07:35.513540Z","shell.execute_reply":"2023-07-15T20:07:35.525305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\n    Defining Deep Emotion architecture based on <a href=\"https://arxiv.org/abs/1902.01019\">paper </a>    </div>","metadata":{}},{"cell_type":"code","source":"class Deep_Emotion(FaceEmotionClassificationBase):\n    def __init__(self):\n\n        super(Deep_Emotion,self).__init__()\n        self.conv1 = nn.Conv2d(3,10,3)\n        self.conv2 = nn.Conv2d(10,10,3)\n        self.pool2 = nn.MaxPool2d(2,2)\n\n        self.conv3 = nn.Conv2d(10,10,3)\n        self.conv4 = nn.Conv2d(10,10,3)\n        self.pool4 = nn.MaxPool2d(2,2)\n\n        self.norm = nn.BatchNorm2d(10)\n\n        self.fc1 = nn.Linear(810,50)\n        self.fc2 = nn.Linear(50,7)\n\n        self.localization = nn.Sequential(\n            nn.Conv2d(3, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        self.fc_loc = nn.Sequential(\n            nn.Linear(640, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 3 * 2)\n        )\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n    def attention(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 640)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n        return x\n\n    def forward(self,input):\n        out = self.attention(input)\n\n        out = F.relu(self.conv1(out))\n        out = self.conv2(out)\n        out = F.relu(self.pool2(out))\n\n        out = F.relu(self.conv3(out))\n        out = self.norm(self.conv4(out))\n        out = F.relu(self.pool4(out))\n\n        out = F.dropout(out)\n        out = out.view(-1, 810)\n        out = F.relu(self.fc1(out))\n        out = self.fc2(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:07:35.528317Z","iopub.execute_input":"2023-07-15T20:07:35.528710Z","iopub.status.idle":"2023-07-15T20:07:35.550060Z","shell.execute_reply.started":"2023-07-15T20:07:35.528669Z","shell.execute_reply":"2023-07-15T20:07:35.549022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nMoving tensor to device   </div>","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Deep_Emotion().to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:07:35.551570Z","iopub.execute_input":"2023-07-15T20:07:35.552814Z","iopub.status.idle":"2023-07-15T20:07:38.098643Z","shell.execute_reply.started":"2023-07-15T20:07:35.552760Z","shell.execute_reply":"2023-07-15T20:07:38.097578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nEvaluation and accuracy functions   </div>","metadata":{}},{"cell_type":"code","source":"#@torch.no_grad\ndef evaluate(model,val_loader):\n    model.eval()\n    outputs =[model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\noptimizer = torch.optim.Adam(model.parameters(),lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:07:38.100413Z","iopub.execute_input":"2023-07-15T20:07:38.100833Z","iopub.status.idle":"2023-07-15T20:07:38.109874Z","shell.execute_reply.started":"2023-07-15T20:07:38.100776Z","shell.execute_reply":"2023-07-15T20:07:38.107631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nDefining fit function    </div>","metadata":{}},{"cell_type":"code","source":"def fit(num_epochs, model, train_loader,val_loader,opt=optimizer):\n    history=[]\n    for epoch in range(num_epochs):\n        model.train()\n        train_losses=[]\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n        print(\"Epoch-{},Loss-{}\".format(epoch,loss.item()))\n\n        result = evaluate(model,val_loader)\n        result['train_loss'] = sum(train_losses)/len(train_losses)\n        model.epoch_end(epoch,result)\n        history.append(result)\n    \n    return history","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:11:22.603874Z","iopub.execute_input":"2023-07-15T20:11:22.604395Z","iopub.status.idle":"2023-07-15T20:11:22.614993Z","shell.execute_reply.started":"2023-07-15T20:11:22.604356Z","shell.execute_reply":"2023-07-15T20:11:22.613764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nTraining the model for 50 epochs   </div>","metadata":{}},{"cell_type":"code","source":"num_epochs=250\nhistory =fit(num_epochs, model, train_loader,val_loader,opt=optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:46:30.912106Z","iopub.execute_input":"2023-07-15T20:46:30.913130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nPlotting accuracy and loss curves for val set    </div>","metadata":{}},{"cell_type":"code","source":"accuracy = [result['val_acc'] for result in history]\nplt.plot(accuracy,'-x')\nplt.xlabel('epoch->')\nplt.ylabel('accuracy->')\nplt.title('Accuracy plot')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:26:18.679102Z","iopub.execute_input":"2023-07-15T20:26:18.680113Z","iopub.status.idle":"2023-07-15T20:26:18.907001Z","shell.execute_reply.started":"2023-07-15T20:26:18.680056Z","shell.execute_reply":"2023-07-15T20:26:18.905768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = [result['val_loss'] for result in history]\nplt.plot(loss,'-x')\nplt.xlabel('epoch ->')\nplt.ylabel('loss ->')\nplt.title('Accuracy plot')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:26:19.531585Z","iopub.execute_input":"2023-07-15T20:26:19.532337Z","iopub.status.idle":"2023-07-15T20:26:19.760834Z","shell.execute_reply.started":"2023-07-15T20:26:19.532297Z","shell.execute_reply":"2023-07-15T20:26:19.759766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nSaving the trained model    </div>","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'face_emotion_deep_emotion.pth')\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:26:22.130547Z","iopub.execute_input":"2023-07-15T20:26:22.130976Z","iopub.status.idle":"2023-07-15T20:26:22.143364Z","shell.execute_reply.started":"2023-07-15T20:26:22.130939Z","shell.execute_reply":"2023-07-15T20:26:22.142189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ\nInferencing the trained model   </div>","metadata":{}},{"cell_type":"code","source":"test_image = cv2.imread('/kaggle/input/fer2013/test/fear/PrivateTest_25595121.jpg')\nplt.imshow(test_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:30:59.012648Z","iopub.execute_input":"2023-07-15T20:30:59.013348Z","iopub.status.idle":"2023-07-15T20:30:59.201048Z","shell.execute_reply.started":"2023-07-15T20:30:59.013309Z","shell.execute_reply":"2023-07-15T20:30:59.199887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\ntest_image = cv2.resize(test_image, (48, 48))\ntest_tensor = transforms.ToTensor()(test_image)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:30:59.878842Z","iopub.execute_input":"2023-07-15T20:30:59.879733Z","iopub.status.idle":"2023-07-15T20:30:59.890359Z","shell.execute_reply.started":"2023-07-15T20:30:59.879693Z","shell.execute_reply":"2023-07-15T20:30:59.889217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]  \nstd = [0.229, 0.224, 0.225] \ntest_tensor = transforms.Normalize(mean, std)(test_tensor)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:31:09.407364Z","iopub.execute_input":"2023-07-15T20:31:09.408043Z","iopub.status.idle":"2023-07-15T20:31:09.419343Z","shell.execute_reply.started":"2023-07-15T20:31:09.408002Z","shell.execute_reply":"2023-07-15T20:31:09.418312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tensor = torch.unsqueeze(test_tensor, 0)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:31:13.918001Z","iopub.execute_input":"2023-07-15T20:31:13.918444Z","iopub.status.idle":"2023-07-15T20:31:13.924469Z","shell.execute_reply.started":"2023-07-15T20:31:13.918392Z","shell.execute_reply":"2023-07-15T20:31:13.923255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    test_tensor = test_tensor.to(device)\n    output = model(test_tensor)\n    probabilities = F.softmax(output, dim=1)\n    predicted_class_index = torch.argmax(probabilities, dim=1).item()\n    predicted_class = reversed_mappings[predicted_class_index]\nprint(\"Predicted Class is\",predicted_class)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T20:31:38.436820Z","iopub.execute_input":"2023-07-15T20:31:38.437547Z","iopub.status.idle":"2023-07-15T20:31:38.449251Z","shell.execute_reply.started":"2023-07-15T20:31:38.437506Z","shell.execute_reply":"2023-07-15T20:31:38.447862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}