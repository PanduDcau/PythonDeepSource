{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54b1005",
   "metadata": {},
   "source": [
    "## Data Acqusition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8162c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f901ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9413</td>\n",
       "      <td>8259</td>\n",
       "      <td>5126</td>\n",
       "      <td>666</td>\n",
       "      <td>1795</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12126</td>\n",
       "      <td>3199</td>\n",
       "      <td>6975</td>\n",
       "      <td>480</td>\n",
       "      <td>3140</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7579</td>\n",
       "      <td>4956</td>\n",
       "      <td>9426</td>\n",
       "      <td>1669</td>\n",
       "      <td>3321</td>\n",
       "      <td>2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5963</td>\n",
       "      <td>3648</td>\n",
       "      <td>6192</td>\n",
       "      <td>425</td>\n",
       "      <td>1716</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6006</td>\n",
       "      <td>11093</td>\n",
       "      <td>18881</td>\n",
       "      <td>1159</td>\n",
       "      <td>7425</td>\n",
       "      <td>2098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh   Milk  Grocery  Frozen  Detergents_Paper  \\\n",
       "0        2       3  12669   9656     7561     214              2674   \n",
       "1        2       3   7057   9810     9568    1762              3293   \n",
       "2        2       3   6353   8808     7684    2405              3516   \n",
       "3        1       3  13265   1196     4221    6404               507   \n",
       "4        2       3  22615   5410     7198    3915              1777   \n",
       "5        2       3   9413   8259     5126     666              1795   \n",
       "6        2       3  12126   3199     6975     480              3140   \n",
       "7        2       3   7579   4956     9426    1669              3321   \n",
       "8        1       3   5963   3648     6192     425              1716   \n",
       "9        2       3   6006  11093    18881    1159              7425   \n",
       "\n",
       "   Delicassen  \n",
       "0        1338  \n",
       "1        1776  \n",
       "2        7844  \n",
       "3        1788  \n",
       "4        5185  \n",
       "5        1451  \n",
       "6         545  \n",
       "7        2566  \n",
       "8         750  \n",
       "9        2098  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/Asus/Documents/Atmel Studio/WholesaleCustomer.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb36a56",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Are there any data points considered outliers for more than one feature based on the definition above? Should these data points be removed from the dataset? If any data points were added to the outliers list to be removed, explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a9b0e",
   "metadata": {},
   "source": [
    "### Feature Transformation\n",
    "In this section you will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bedabb",
   "metadata": {},
   "source": [
    "### Implementation: PCA\n",
    "Now that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the good_data to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the explained variance ratio of each dimension — how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a59519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4f1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac8e7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen samples of wholesale customers dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>630</td>\n",
       "      <td>11095</td>\n",
       "      <td>23998</td>\n",
       "      <td>787</td>\n",
       "      <td>9529</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>31714</td>\n",
       "      <td>12319</td>\n",
       "      <td>11757</td>\n",
       "      <td>287</td>\n",
       "      <td>3881</td>\n",
       "      <td>2931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>56159</td>\n",
       "      <td>555</td>\n",
       "      <td>902</td>\n",
       "      <td>10002</td>\n",
       "      <td>212</td>\n",
       "      <td>2916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh   Milk  Grocery  Frozen  Detergents_Paper  \\\n",
       "0        2       3    630  11095    23998     787              9529   \n",
       "1        2       3  31714  12319    11757     287              3881   \n",
       "2        1       3  56159    555      902   10002               212   \n",
       "\n",
       "   Delicassen  \n",
       "0          72  \n",
       "1        2931  \n",
       "2        2916  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Select three indices of your choice you wish to sample from the dataset\n",
    "indices = [43, 12, 39]\n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "# .reset_index(drop = True) resets the index from 0, 1 and 2 instead of 100, 200 and 300 \n",
    "samples = pd.DataFrame(data.loc[indices], columns = data.columns).reset_index(drop = True)\n",
    "print (\"Chosen samples of wholesale customers dataset:\")\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911ba7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale the data using the natural logarithm\n",
    "log_data = np.log(data)\n",
    "\n",
    "# TODO: Scale the sample data using the natural logarithm\n",
    "log_samples = np.log(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897f4834",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'Channel':\n",
      "Data points considered outliers for the feature 'Region':\n",
      "Data points considered outliers for the feature 'Fresh':\n",
      "Data points considered outliers for the feature 'Milk':\n",
      "Data points considered outliers for the feature 'Grocery':\n",
      "Data points considered outliers for the feature 'Frozen':\n",
      "Data points considered outliers for the feature 'Detergents_Paper':\n",
      "Data points considered outliers for the feature 'Delicassen':\n",
      "Outliers list:\n",
      " [38, 57, 65, 66, 75, 81, 86, 95, 96, 98, 109, 128, 137, 142, 145, 154, 161, 171, 175, 183, 184, 187, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 285, 289, 304, 305, 325, 338, 343, 353, 355, 356, 357, 412, 420, 429, 439]\n",
      "Length of outliers list:\n",
      " 115\n",
      "Duplicate list:\n",
      " [128, 65, 66, 154, 264, 233, 203, 75, 218]\n",
      "Length of duplicates list:\n",
      " 9\n",
      "Original shape of data:\n",
      " (440, 8)\n",
      "New shape of data:\n",
      " (431, 8)\n"
     ]
    }
   ],
   "source": [
    "# Select the indices for data points you wish to remove\n",
    "outliers_lst  = []\n",
    "\n",
    "# For each feature find the data points with extreme high or low values\n",
    "for feature in log_data.columns:\n",
    "    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = np.percentile(log_data.loc[:, feature], 25)\n",
    "\n",
    "    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = np.percentile(log_data.loc[:, feature], 75)\n",
    "\n",
    "    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    step = 1.5 * (Q3 - Q1)\n",
    "\n",
    "    # Display the outliers\n",
    "    print (\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "\n",
    "    # The tilde sign ~ means not\n",
    "    # So here, we're finding any points outside of Q1 - step and Q3 + step\n",
    "    outliers_rows = log_data.loc[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step)), :]\n",
    "    # display(outliers_rows)\n",
    "\n",
    "    outliers_lst.append(list(outliers_rows.index))\n",
    "\n",
    "outliers = list(itertools.chain.from_iterable(outliers_lst))\n",
    "\n",
    "# List of unique outliers\n",
    "# We use set()\n",
    "# Sets are lists with no duplicate entries\n",
    "uniq_outliers = list(set(outliers))\n",
    "\n",
    "# List of duplicate outliers\n",
    "dup_outliers = list(set([x for x in outliers if outliers.count(x) > 1]))\n",
    "\n",
    "print ('Outliers list:\\n', uniq_outliers)\n",
    "print ('Length of outliers list:\\n', len(uniq_outliers))\n",
    "\n",
    "print ('Duplicate list:\\n', dup_outliers)\n",
    "print ('Length of duplicates list:\\n', len(dup_outliers))\n",
    "\n",
    "# Remove duplicate outliers\n",
    "# Only 5 specified\n",
    "good_data = log_data.drop(log_data.index[dup_outliers]).reset_index(drop = True)\n",
    "\n",
    "# Original Data \n",
    "print ('Original shape of data:\\n', data.shape)\n",
    "# Processed Data\n",
    "print ('New shape of data:\\n', good_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e83f9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-object-detection-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cb7dc05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import renders as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65733631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply PCA by fitting the good data with the same number of dimensions as features\n",
    "# Instantiate\n",
    "pca = PCA(n_components=6)\n",
    "# Fit\n",
    "pca.fit(good_data)\n",
    "\n",
    "# TODO: Transform the sample log-data using the PCA fit above\n",
    "pca_samples = pca.transform(log_samples)\n",
    "\n",
    "# Generate PCA results plot\n",
    "#pca_results = rs.pca_results(good_data, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e238122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataFrame of results\n",
    "# display(pca_results)\n",
    "\n",
    "# # DataFrame\n",
    "# display(type(pca_results))\n",
    "\n",
    "# # Cumulative explained variance should add to 1\n",
    "# display(pca_results['Explained Variance'].cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec277727",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "How much variance in the data is explained in total by the first and second principal component? What about the first four principal components? Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending.\n",
    "Hint: A positive increase in a specific dimension corresponds with an increase of the positive-weighted features and a decrease of the negative-weighted features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5be27",
   "metadata": {},
   "source": [
    "**70.68% of the variance in the data is explained by the first and second principal components.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7e191",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions. Observe the numerical value for the first four dimensions of the sample points. Consider if this is consistent with your initial interpretation of the sample points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f40d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display sample log-data after having a PCA transformation applied\n",
    "# display(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc28b03",
   "metadata": {},
   "source": [
    "**Implementation: Dimensionality Reduction**\n",
    "\n",
    "When using **principal component analysis**, one of the main goals is to reduce the dimensionality of the data — in effect, reducing the complexity of the problem. Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the cumulative explained variance ratio is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81da01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "# Instantiate\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(good_data)\n",
    "\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(good_data)\n",
    "\n",
    "# TODO: Transform the sample log-data using the PCA fit above\n",
    "pca_samples = pca.transform(log_samples)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdd26b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dimension 1</th>\n",
       "      <th>Dimension 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.1456</td>\n",
       "      <td>2.7339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.3094</td>\n",
       "      <td>-1.1407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0119</td>\n",
       "      <td>-2.5761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dimension 1  Dimension 2\n",
       "0      -3.1456       2.7339\n",
       "1      -2.3094      -1.1407\n",
       "2       3.0119      -2.5761"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display sample log-data after applying PCA transformation in two dimensions\n",
    "display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767e71e",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "In this section, you will choose to use either a K-Means clustering algorithm or a Gaussian Mixture Model clustering algorithm to identify the various customer segments hidden in the data. You will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0a4f9",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "What are the advantages to using a K-Means clustering algorithm? What are the advantages to using a Gaussian Mixture Model clustering algorithm? Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb889d",
   "metadata": {},
   "source": [
    "#### K-Means Clustering Algorithm\n",
    "\n",
    "Intuition: points in the same cluster has shorter than points from other clusters.\n",
    "The goal is to minimize the distance within the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ebcb1",
   "metadata": {},
   "source": [
    "### Implementation: Creating Clusters\n",
    "Depending on the problem, the number of clusters that you expect to be in the data may already be known. When the number of clusters is not known a priori, there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data — if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's silhouette coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51003571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "model = mixture.GaussianMixture(n_components=3, covariance_type='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9873ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e989c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Create range of clusters \n",
    "range_n_clusters = list(range(2,11))\n",
    "print(range_n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d37e3f",
   "metadata": {},
   "source": [
    "### GMM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bad3815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2. The average silhouette_score is : 0.3806751567489148\n",
      "For n_clusters = 3. The average silhouette_score is : 0.30557750617479806\n",
      "For n_clusters = 4. The average silhouette_score is : 0.23263932935672096\n",
      "For n_clusters = 5. The average silhouette_score is : 0.27370930217073575\n",
      "For n_clusters = 6. The average silhouette_score is : 0.2391983126036898\n",
      "For n_clusters = 7. The average silhouette_score is : 0.2518979226105188\n",
      "For n_clusters = 8. The average silhouette_score is : 0.2962397684575369\n",
      "For n_clusters = 9. The average silhouette_score is : 0.3020063340453915\n",
      "For n_clusters = 10. The average silhouette_score is : 0.3213751406539946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loop through clusters\n",
    "for n_clusters in range_n_clusters:\n",
    "    # TODO: Apply your clustering algorithm of choice to the reduced data \n",
    "    clusterer =  mixture.GaussianMixture(n_components=n_clusters,covariance_type='full').fit(reduced_data)\n",
    "\n",
    "    # TODO: Predict the cluster for each data point\n",
    "    preds = clusterer.predict(reduced_data)\n",
    "\n",
    "    # TODO: Find the cluster centers\n",
    "    centers = clusterer.means_\n",
    "\n",
    "    # TODO: Predict the cluster for each transformed sample data point\n",
    "    sample_preds = clusterer.predict(pca_samples)\n",
    "\n",
    "    # TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "    score = silhouette_score(reduced_data, preds, metric='mahalanobis')\n",
    "    print (\"For n_clusters = {}. The average silhouette_score is : {}\".format(n_clusters, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99d8f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest_bic = np.infty\n",
    "# bic = []\n",
    "# n_components_range = range(1, 7)\n",
    "# cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "# for cv_type in cv_types:\n",
    "#     for n_components in n_components_range:\n",
    "#         # Fit a mixture of Gaussians with EM\n",
    "#         gmm = mixture.GaussianMixture(n_components=n_components, covariance_type=cv_type)\n",
    "#         gmm.fit(X)\n",
    "#         bic.append(gmm.bic(X))\n",
    "#         if bic[-1] < lowest_bic:\n",
    "#             lowest_bic = bic[-1]\n",
    "#             best_gmm = gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ed98c",
   "metadata": {},
   "source": [
    "## KNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7839f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2. The average silhouette_score is : 0.4332792700337813\n",
      "For n_clusters = 3. The average silhouette_score is : 0.39079681464883675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4. The average silhouette_score is : 0.33470397712183914\n",
      "For n_clusters = 5. The average silhouette_score is : 0.35300221267118004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 6. The average silhouette_score is : 0.35716865839190426\n",
      "For n_clusters = 7. The average silhouette_score is : 0.35924879094431256\n",
      "For n_clusters = 8. The average silhouette_score is : 0.35302264182072457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 9. The average silhouette_score is : 0.34444907519167944\n",
      "For n_clusters = 10. The average silhouette_score is : 0.3557552419490541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loop through clusters\n",
    "for n_clusters in range_n_clusters:\n",
    "    # TODO: Apply your clustering algorithm of choice to the reduced data \n",
    "    clusterer = KMeans(n_clusters=n_clusters).fit(reduced_data)\n",
    "\n",
    "    # TODO: Predict the cluster for each data point\n",
    "    preds = clusterer.predict(reduced_data)\n",
    "\n",
    "    # TODO: Find the cluster centers\n",
    "    centers = clusterer.cluster_centers_\n",
    "\n",
    "    # TODO: Predict the cluster for each transformed sample data point\n",
    "    sample_preds = clusterer.predict(pca_samples)\n",
    "\n",
    "    # TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "    score = silhouette_score(reduced_data, preds, metric='euclidean')\n",
    "    print (\"For n_clusters = {}. The average silhouette_score is : {}\".format(n_clusters, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ab2c2",
   "metadata": {},
   "source": [
    "### Distance Metric\n",
    "\n",
    "The Silhouette Coefficient is calculated using the mean intra-cluster distance and the mean nearest-cluster distance for each sample. \n",
    "\n",
    "Therefore, it makes sense to use the same distance metric here as the one used in the clustering algorithm. This is Euclidean for KMeans and Mahalanobis for general GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77da152",
   "metadata": {},
   "source": [
    "### Cluster Visualization\n",
    "Once you've chosen the optimal number of clusters for your clustering algorithm using the scoring metric above, you can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c17fba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GaussianMixture was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Extra code because we ran a loop on top and this resets to what we want\n",
    "clusterer = mixture.GaussianMixture(n_components=2).fit(reduced_data)\n",
    "preds = clusterer.predict(reduced_data)\n",
    "centers = clusterer.means_\n",
    "sample_preds = clusterer.predict(pca_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39544ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results of the clustering from implementation\n",
    "# rs.cluster_results(reduced_data, preds, centers, pca_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa297f3",
   "metadata": {},
   "source": [
    "### Implementation: Data Recovery\n",
    "Each cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the averages of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to the average customer of that segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be978afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Segment 0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9164.0</td>\n",
       "      <td>2101.0</td>\n",
       "      <td>2737.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Segment 1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>7818.0</td>\n",
       "      <td>12216.0</td>\n",
       "      <td>870.0</td>\n",
       "      <td>4715.0</td>\n",
       "      <td>947.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Channel  Region   Fresh    Milk  Grocery  Frozen  Detergents_Paper  \\\n",
       "Segment 0      1.0     2.0  9164.0  2101.0   2737.0  2120.0             345.0   \n",
       "Segment 1      2.0     2.0  3704.0  7818.0  12216.0   870.0            4715.0   \n",
       "\n",
       "           Delicassen  \n",
       "Segment 0       754.0  \n",
       "Segment 1       947.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Inverse transform the centers\n",
    "log_centers = pca.inverse_transform(centers)\n",
    "\n",
    "# TODO: Exponentiate the centers\n",
    "true_centers = np.exp(log_centers)\n",
    "\n",
    "# Display the true centers\n",
    "segments = ['Segment {}'.format(i) for i in range(0,len(centers))]\n",
    "true_centers = pd.DataFrame(np.round(true_centers), columns = data.columns)\n",
    "true_centers.index = segments\n",
    "display(true_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9384ee",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Consider the total purchase cost of each product category for the representative data points above, and reference the statistical description of the dataset at the beginning of this project. What set of establishments could each of the customer segments represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3927cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Segment 0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>-1526.0</td>\n",
       "      <td>-2018.5</td>\n",
       "      <td>594.0</td>\n",
       "      <td>-471.5</td>\n",
       "      <td>-211.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Segment 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-4800.0</td>\n",
       "      <td>4191.0</td>\n",
       "      <td>7460.5</td>\n",
       "      <td>-656.0</td>\n",
       "      <td>3898.5</td>\n",
       "      <td>-18.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Channel  Region   Fresh    Milk  Grocery  Frozen  Detergents_Paper  \\\n",
       "Segment 0      0.0    -1.0   660.0 -1526.0  -2018.5   594.0            -471.5   \n",
       "Segment 1      1.0    -1.0 -4800.0  4191.0   7460.5  -656.0            3898.5   \n",
       "\n",
       "           Delicassen  \n",
       "Segment 0      -211.5  \n",
       "Segment 1       -18.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Segment 0</th>\n",
       "      <td>-0.322727</td>\n",
       "      <td>-0.543182</td>\n",
       "      <td>-2836.297727</td>\n",
       "      <td>-3695.265909</td>\n",
       "      <td>-5214.277273</td>\n",
       "      <td>-951.931818</td>\n",
       "      <td>-2536.493182</td>\n",
       "      <td>-770.870455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Segment 1</th>\n",
       "      <td>0.677273</td>\n",
       "      <td>-0.543182</td>\n",
       "      <td>-8296.297727</td>\n",
       "      <td>2021.734091</td>\n",
       "      <td>4264.722727</td>\n",
       "      <td>-2201.931818</td>\n",
       "      <td>1833.506818</td>\n",
       "      <td>-577.870455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Channel    Region        Fresh         Milk      Grocery  \\\n",
       "Segment 0 -0.322727 -0.543182 -2836.297727 -3695.265909 -5214.277273   \n",
       "Segment 1  0.677273 -0.543182 -8296.297727  2021.734091  4264.722727   \n",
       "\n",
       "                Frozen  Detergents_Paper  Delicassen  \n",
       "Segment 0  -951.931818      -2536.493182 -770.870455  \n",
       "Segment 1 -2201.931818       1833.506818 -577.870455  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clusters' deviation from median\n",
    "display(true_centers - data.median())\n",
    "\n",
    "# Clusters' deviation from mean\n",
    "# As you can see, this is not a meaningful comparison for Segment 1 where everything is negative\n",
    "display(true_centers - data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac94a56",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We will be using deviations from the median, with reference to the statistical description of the dataset at the beginning of this project, since mean is sensitive to outliers and would not yield meaningful comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34255cdf",
   "metadata": {},
   "source": [
    "**Question 9**\n",
    "\n",
    "For each sample point, which customer segment from Question 8 best represents it? Are the predictions for each sample point consistent with this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fbc337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample point 0 predicted to be in Cluster 1\n",
      "Sample point 1 predicted to be in Cluster 1\n",
      "Sample point 2 predicted to be in Cluster 0\n"
     ]
    }
   ],
   "source": [
    "# Display the predictions\n",
    "for i, pred in enumerate(sample_preds):\n",
    "    print (\"Sample point\", i, \"predicted to be in Cluster\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a35aab50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>630</td>\n",
       "      <td>11095</td>\n",
       "      <td>23998</td>\n",
       "      <td>787</td>\n",
       "      <td>9529</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>31714</td>\n",
       "      <td>12319</td>\n",
       "      <td>11757</td>\n",
       "      <td>287</td>\n",
       "      <td>3881</td>\n",
       "      <td>2931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>56159</td>\n",
       "      <td>555</td>\n",
       "      <td>902</td>\n",
       "      <td>10002</td>\n",
       "      <td>212</td>\n",
       "      <td>2916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh   Milk  Grocery  Frozen  Detergents_Paper  \\\n",
       "0        2       3    630  11095    23998     787              9529   \n",
       "1        2       3  31714  12319    11757     287              3881   \n",
       "2        1       3  56159    555      902   10002               212   \n",
       "\n",
       "   Delicassen  \n",
       "0          72  \n",
       "1        2931  \n",
       "2        2916  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09d6e4",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "**Sample 1:**\n",
    "It is evident that this belongs to cluster 0 (segment 0) where spending on \"Milk\", \"Grocery\" And \"Detergents_Paper\" is high.\n",
    "\n",
    "**Sample 2:**\n",
    "This is tricky. The spending on \"Milk\", \"Grocery\" and \"Detergents_Paper\" is high. But spending on \"Fresh\" is high too.\n",
    "Considering spending on \"Frozen\" is low, I guess it makes sense to cluster it under cluster 0.\n",
    "\n",
    "**Sample 3:**\n",
    "It is evident that this belongs to cluster 1 because spending on \"Fresh\" and \"Frozen\" is high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b41dfd",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this final section, you will investigate ways that you can make use of the clustered data. First, you will consider how the different groups of customers, the customer segments, may be affected differently by a specific delivery scheme.\n",
    "\n",
    "Next, you will consider how giving a label to each customer (which segment that customer belongs to) can provide for additional features about the customer data. Finally, you will compare the customer segments to a hidden variable present in the data, to see whether the clustering identified certain relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c9e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
