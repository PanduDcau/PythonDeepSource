{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nltk files to be downloaded\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Corpus files\n",
    "file = nltk.corpus.gutenberg.fileids() [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file)\n",
    "emmatokens = nltk.wordpunct_tokenize(emmatext)\n",
    "emmawords = [w.lower() for w in emmatokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "ancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenti', '-', 'one', 'year', 'in', 'the', 'world', 'with', 'veri', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'wa', 'the', 'youngest', 'of', 'the', 'two', 'daughter', 'of', 'a', 'most', 'affection', ',', 'indulg', 'father', ';', 'and', 'had', ',', 'in', 'consequ', 'of', 'her', 'sister', \"'\", 's', 'marriag', ',', 'been', 'mistress', 'of', 'hi', 'hous', 'from', 'a', 'veri', 'earli', 'period', '.', 'her']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization files\n",
    "emmaregstem = [porter.stem(t) for t in emmatokens]\n",
    "print(emmaregstem[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenti', '-', 'one', 'year', 'in', 'the', 'world', 'with', 'veri', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'wa', 'the', 'youngest', 'of', 'the', 'two', 'daughter', 'of', 'a', 'most', 'affection', ',', 'indulg', 'father', ';', 'and', 'had', ',', 'in', 'consequ', 'of', 'her', 'sister', \"'\", 's', 'marriag', ',', 'been', 'mistress', 'of', 'hi', 'hous', 'from', 'a', 'veri', 'earli', 'period', '.', 'her']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization in lowerstems\n",
    "emmalowerstem = [porter.stem(t) for t in emmawords]\n",
    "print(emmalowerstem[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emm', 'by', 'jan', 'aust', '1816', ']', 'volum', 'i', 'chapt', 'i', 'emm', 'woodh', ',', 'handsom', ',', 'clev', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'hom', 'and', 'happy', 'disposit', ',', 'seem', 'to', 'unit', 'som', 'of', 'the', 'best', 'bless', 'of', 'ex', ';', 'and', 'had', 'liv', 'near', 'twenty', '-', 'on', 'year', 'in', 'the', 'world', 'with', 'very', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daught', 'of', 'a', 'most', 'affect', ',', 'indulg', 'fath', ';', 'and', 'had', ',', 'in', 'consequ', 'of', 'her', 'sist', \"'\", 's', 'marry', ',', 'been', 'mistress', 'of', 'his', 'hous', 'from', 'a', 'very', 'ear', 'period', '.', 'her']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization regstems\n",
    "emmaregstem = [ancaster.stem(t) for t in emmatokens]\n",
    "print(emmaregstem[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emm', 'by', 'jan', 'aust', '1816', ']', 'volum', 'i', 'chapt', 'i', 'emm', 'woodh', ',', 'handsom', ',', 'clev', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'hom', 'and', 'happy', 'disposit', ',', 'seem', 'to', 'unit', 'som', 'of', 'the', 'best', 'bless', 'of', 'ex', ';', 'and', 'had', 'liv', 'near', 'twenty', '-', 'on', 'year', 'in', 'the', 'world', 'with', 'very', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daught', 'of', 'a', 'most', 'affect', ',', 'indulg', 'fath', ';', 'and', 'had', ',', 'in', 'consequ', 'of', 'her', 'sist', \"'\", 's', 'marry', ',', 'been', 'mistress', 'of', 'his', 'hous', 'from', 'a', 'very', 'ear', 'period', '.', 'her']\n"
     ]
    }
   ],
   "source": [
    "emmalowerstem = [ancaster.stem(t) for t in emmawords]\n",
    "print(emmalowerstem[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'friend'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words seperation\n",
    "def stem(word):\n",
    "    for suffix in ['ing','ly','ed','ious','ies','ive','es','s']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "stemmedword = stem('friends')\n",
    "stemmedword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessing', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'year', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'wa', 'the', 'youngest', 'of', 'the', 'two', 'daughter', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her']\n"
     ]
    }
   ],
   "source": [
    "#filtering Lemmas\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "emmalemma=[wnl.lemmatize(t) for t in emmawords]\n",
    "print(emmalemma[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emmatext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887071"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emmatext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print full shorttext\n",
    "shorttext = emmatext[:150]\n",
    "shorttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "E\n",
      "m\n",
      "m\n",
      "a\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "J\n"
     ]
    }
   ],
   "source": [
    "for char in shorttext[:10]:\n",
    "    print (char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty PythonHoly Grail'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1 = 'Monty Python'\n",
    "string2 = 'Holy Grail'\n",
    "string1 + string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python and the Holy Grail'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing string concatanation\n",
    "string1 + ' and the ' + string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newemmatext = emmatext.replace('\\n', ' ')\n",
    "shorttext = newemmatext[:150]\n",
    "shorttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " '1816',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " 'handsome',\n",
       " 'clever',\n",
       " 'and',\n",
       " 'rich',\n",
       " 'with',\n",
       " 'a',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'and',\n",
       " 'happy',\n",
       " 'disposition',\n",
       " 'seemed',\n",
       " 'to']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pword = re.compile('\\w+')\n",
    "re.findall(pword, shorttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U', 'S', 'A', 'poster', 'print', 'costs', '12', '40', 'with', '10', 'off']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtext = 'U.S.A. poster-print costs $12.40, with 10% off.'\n",
    "re.findall(pword, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U', ''),\n",
       " ('S', ''),\n",
       " ('A', ''),\n",
       " ('poster-print', '-print'),\n",
       " ('costs', ''),\n",
       " ('12', ''),\n",
       " ('40', ''),\n",
       " ('with', ''),\n",
       " ('10', ''),\n",
       " ('off', '')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptoken = re.compile('(\\w+(-\\w+)*)')\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pattern Identification\n",
    "pabbrev = re.compile('(([A-Z]\\.)+)')\n",
    "re.findall(pabbrev, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U', '', ''),\n",
       " ('S', '', ''),\n",
       " ('A', '', ''),\n",
       " ('poster-print', '-print', ''),\n",
       " ('costs', '', ''),\n",
       " ('12', '', ''),\n",
       " ('40', '', ''),\n",
       " ('with', '', ''),\n",
       " ('10', '', ''),\n",
       " ('off', '', '')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding All \n",
    "ptoken = re.compile('(\\w+(-\\w+)*|([A-Z]\\.)+)')\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.', ''),\n",
       " ('poster-print', '', '-print'),\n",
       " ('costs', '', ''),\n",
       " ('12', '', ''),\n",
       " ('40', '', ''),\n",
       " ('with', '', ''),\n",
       " ('10', '', ''),\n",
       " ('off', '', '')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pattern 2 identification\n",
    "ptoken = re.compile('(([A-Z]\\.)+|\\w+(-\\w+)*)')\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.', '', ''),\n",
       " ('poster-print', '', '-print', ''),\n",
       " ('costs', '', '', ''),\n",
       " ('$12.40', '', '', '.40'),\n",
       " ('with', '', '', ''),\n",
       " ('10', '', '', ''),\n",
       " ('off', '', '', '')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptoken = re.compile(r'(([A-Z]\\.)+|\\w+(-\\w+)*|\\$?\\d+(\\.\\d+)?)')\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A.', '', ''),\n",
       " ('', '-print', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '.40'),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptoken = re.compile(r'''([A-Z]\\.)+ \n",
    "    | \\w+(-\\w+)* \n",
    "    | \\$?\\d+(\\.\\d+)? \n",
    "    ''', re.X)\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'''(?x)        # set flag to allow verbose regexps\n",
    "      |(?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.             # ellipsis\n",
    "      | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[',\n",
       " '',\n",
       " 'Emma',\n",
       " '',\n",
       " '',\n",
       " 'by',\n",
       " '',\n",
       " '',\n",
       " 'Jane',\n",
       " '',\n",
       " '',\n",
       " 'Austen',\n",
       " '',\n",
       " '',\n",
       " '1816',\n",
       " '',\n",
       " ']',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'VOLUME',\n",
       " '',\n",
       " '',\n",
       " 'I',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'CHAPTER',\n",
       " '',\n",
       " '',\n",
       " 'I',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Emma',\n",
       " '',\n",
       " '',\n",
       " 'Woodhouse',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '',\n",
       " 'handsome',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '',\n",
       " 'clever',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '',\n",
       " 'and',\n",
       " '',\n",
       " '',\n",
       " 'rich',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '',\n",
       " 'with',\n",
       " '',\n",
       " '',\n",
       " 'a',\n",
       " '',\n",
       " '',\n",
       " 'comfortable',\n",
       " '',\n",
       " '',\n",
       " 'home',\n",
       " '',\n",
       " '',\n",
       " 'and',\n",
       " '',\n",
       " '',\n",
       " 'happy',\n",
       " '',\n",
       " '',\n",
       " 'disposition',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '',\n",
       " 'seemed',\n",
       " '',\n",
       " '',\n",
       " 'to',\n",
       " '']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern = r''' (?x) \n",
    "#     ([A-Z]\\.)+ \n",
    "#     | \\w+(-\\w+)* \n",
    "#     | \\$?\\d+(\\.\\d+)?%? \n",
    "#     | \\.\\.\\.\n",
    "#     | [][.,;\"'?():-_']\n",
    "#     '''\n",
    "\n",
    "shorttext = emmatext[:150]\n",
    "shorttext\n",
    "nltk.regexp_tokenize(shorttext, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'U.S.A.',\n",
       " '',\n",
       " '',\n",
       " 'poster-print',\n",
       " '',\n",
       " '',\n",
       " 'costs',\n",
       " '',\n",
       " '',\n",
       " '$12.40',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '',\n",
       " 'with',\n",
       " '',\n",
       " '',\n",
       " '10',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'off',\n",
       " '',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(specialtext, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetPattern = r'''(?x)    \n",
    "    (?:https?://|www)\\S+ \n",
    "    | (?::-\\) | ;-\\))\n",
    "    | &(?:amp|lt|gt|quot);\n",
    "    | \\#\\w+\n",
    "    | @\\w+\n",
    "    | \\d+:\\d+\n",
    "    | \\d+\\.\\d+\n",
    "    | (?:\\d+,)+?\\d{3}(?:=(?:[^,]|$))\n",
    "    | (?:[A-Z]\\.)+\n",
    "    | (?:--+)\n",
    "    | \\w+(?:-\\w+)*\n",
    "    | ['\\\".?!,:;]+\n",
    "    '''  \n",
    "\n",
    "tweepat = r'''(?x)\n",
    "    (?:[A-Z]\\.)+\n",
    "    | @\\w+\n",
    "    | \\d+:\\d+\n",
    "    | \\d+\\.\\d+\n",
    "    | ['\\.\".?!,:;]+\n",
    "    | \\#\\w+\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@natalieohayre', '#hc09', '!', '#tcot', '#fishy', \"'\", '!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1 = \"@natalieohayre I agree #hc09 needs reform- but not by crooked politicians who r clueless about healthcare! #tcot #fishy NO GOV'T TAKEOVER!\"\n",
    "tweet2 = \"To Sen. Roland Burris: Affordable, quality health insurance can't wait http://bit.ly/j63je #hc09 #IL #60660\"\n",
    "tweet3 = \"RT @karoli: RT @Seriou: .@whitehouse I will stand w/ Obama on #healthcare, I trust him. #p2 #tlot\"\n",
    "\n",
    "nltk.regexp_tokenize(tweet1,tweepat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@natalieohayre',\n",
       " 'I',\n",
       " 'agree',\n",
       " '#hc09',\n",
       " 'needs',\n",
       " 'reform',\n",
       " 'but',\n",
       " 'not',\n",
       " 'by',\n",
       " 'crooked',\n",
       " 'politicians',\n",
       " 'who',\n",
       " 'r',\n",
       " 'clueless',\n",
       " 'about',\n",
       " 'healthcare',\n",
       " '!',\n",
       " '#tcot',\n",
       " '#fishy',\n",
       " 'NO',\n",
       " 'GOV',\n",
       " \"'\",\n",
       " 'T',\n",
       " 'TAKEOVER',\n",
       " '!']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(tweet1,tweetPattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', ':', ',', \"'\", ':', '.', '#hc09', '#IL', '#60660']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only Printing Hashtags and mentions \n",
    "nltk.regexp_tokenize(tweet2,tweepat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'Sen',\n",
       " '.',\n",
       " 'Roland',\n",
       " 'Burris',\n",
       " ':',\n",
       " 'Affordable',\n",
       " ',',\n",
       " 'quality',\n",
       " 'health',\n",
       " 'insurance',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'wait',\n",
       " 'http://bit.ly/j63je',\n",
       " '#hc09',\n",
       " '#IL',\n",
       " '#60660']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(tweet2,tweetPattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@karoli',\n",
       " ':',\n",
       " '@Seriou',\n",
       " ':',\n",
       " '.',\n",
       " '@whitehouse',\n",
       " '#healthcare',\n",
       " ',',\n",
       " '.',\n",
       " '#p2',\n",
       " '#tlot']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(tweet3,tweepat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@karoli',\n",
       " ':',\n",
       " 'RT',\n",
       " '@Seriou',\n",
       " ':',\n",
       " '.',\n",
       " '@whitehouse',\n",
       " 'I',\n",
       " 'will',\n",
       " 'stand',\n",
       " 'w',\n",
       " 'Obama',\n",
       " 'on',\n",
       " '#healthcare',\n",
       " ',',\n",
       " 'I',\n",
       " 'trust',\n",
       " 'him',\n",
       " '.',\n",
       " '#p2',\n",
       " '#tlot']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(tweet3,tweetPattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
