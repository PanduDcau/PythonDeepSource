{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fc3db9",
   "metadata": {},
   "source": [
    "## Introduction to the Project\n",
    "* References\n",
    "* Dataset Information\n",
    "* Some Notebooks For Tips\n",
    "\n",
    "#### 1. Refrences\n",
    "This dataset is from Kaggle, under their begginer **Machine Learning** Projects section. And yes indeed, this is a very starightforward and an easy to project to walkthrough, yet giving you a good grasp on the basics of Machine Learning as well as Data Science. However mainly **Machine Learning**.\n",
    "\n",
    "Here is the dataset link: `https://www.kaggle.com/competitions/titanic/data`\n",
    "\n",
    "#### 2. Dataset Information\n",
    "There is nothing much to know about the dataset, it's only the data dictionary you may need which is available on the Kaggle [Project Website](https://www.kaggle.com/competitions/titanic/data). The information on the website is very well explaing the dataset you'll be working with\n",
    "\n",
    "#### 3. Some Notebooks For Tips\n",
    "* [A Classy Clash and a Classic Classification ðŸ§ŠðŸš¢](https://www.kaggle.com/code/chazzer/a-classy-clash-and-a-classic-classification)\n",
    "* [[Score 0.79425] Titanic EDA + Prediction Explained](https://www.kaggle.com/code/rushikeshsawarkar/score-0-79425-titanic-eda-prediction-explained#Titanic-Survival-EDA-and-Prediction-Explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe260af",
   "metadata": {},
   "source": [
    "## Project Table of Contents\n",
    "* Doing some Exploratory Data Analysis (EDA)\n",
    "* Feature Engineering\n",
    "* Modelling\n",
    "* Some References (Articles/Blogs, Docs etc.)\n",
    "    \n",
    "    \n",
    "#### 1. EDA\n",
    "1. Finding the most Correlated features to our dataset\n",
    "2. Getting some frequencies/counts of the features\n",
    "3. Not to forget to create some relational plots (Column VS this Column)\n",
    "\n",
    "\n",
    "#### 2. Feature Engineering\n",
    "1. Changing some names of columns\n",
    "2. Filling `NULL/NAN` values\n",
    "3. Deleting & Merging some columns\n",
    "4. Applying `OneHotEncoding` before moving to modelling\n",
    "\n",
    "#### 3. Modelling\n",
    "1. Trying different models out like `Logistic Regression`, `KNN` and `Random Forest`\n",
    "2. Scoring our model BEFORE tuning (Cross Validation)\n",
    "3. Trying to improve all of these models with `RandomizedSearchCV`, `GridSearchCV` and `XGBoost`\n",
    "4. Using Cross Validation to check the result of our predictions (AFTER TUNE)\n",
    "5. Finally plotting a heatmap to show the best model out of the above mentioned 3, and use it to make our final predictions which, then we'll submit to **Kaggle**\n",
    "\n",
    "#### 4. Some References\n",
    "* [XGBoost Tuning](https://www.section.io/engineering-education/machine-learning-with-xgboost-and-scikit-learn/)\n",
    "* [Pre-Processing / Feature Engineering](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)\n",
    "\n",
    "##### Looks pretty dope if you ask me, I'm super interested and I hope you are too! Let's get started!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
